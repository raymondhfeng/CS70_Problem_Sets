\documentclass[11pt]{article}

\usepackage{amsmath,amssymb,amsthm,setspace,tabto,fancyhdr,sectsty,mathtools}
\usepackage[shortlabels]{enumitem}
\usepackage[nobreak=true]{mdframed}
\usepackage[left=1.25in,right=0.75in,top=1.25in,bottom=2.0in]{geometry}

\newcommand*{\Question}[1]{\section{#1}}
\newenvironment{Parts}{\begin{enumerate}[label=(\alph*)]}{\end{enumerate}}
\newcommand*{\Part}{\item}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% UNCOMMENT NEXT LINE FOR SOLUTION BOXES
% \newcommand*{\solnboxes}{}

%%%%%%%%%%%%%%%%%%%% name/id
\rfoot{\small Raymond Feng | 3032021864}

%%%%%%%%%%%%%%%%%%%% hw number
\newcommand*{\hwnum}{9}


\ifdefined\solnboxes
    \newenvironment{Answer}{\vspace{10pt}\begin{mdframed}\textbf{Solution}\\}{\end{mdframed}\vfill\pagebreak[3]}
\else
    \newenvironment{Answer}{\vspace{10pt}}{\vfill\pagebreak[3]}
\fi
\newcommand*{\MC}[1]{\multicolumn{1}{c}{#1}}
\newcommand*{\N}{\mathbb{N}}
\newcommand*{\Z}{\mathbb{Z}}
\newcommand*{\Q}{\mathbb{Q}}
\newcommand*{\R}{\mathbb{R}}
\newcommand*{\C}{\mathbb{C}}
\newcommand*{\GF}{\text{GF}}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}

\pagestyle{fancy}
\headheight=75pt
\sectionfont{\Large\fontfamily{lmdh}\selectfont}

\renewcommand{\headrulewidth}{6pt}
\chead{\rule{\textwidth}{6pt} \vspace{20pt}\\}
\lhead{\setstretch{1.05}\Large\fontfamily{lmdh}\selectfont
CS 70        \tabto{96pt} Discrete Mathematics and Probability Theory\smallskip\\
Spring 2017  \tabto{96pt} Rao}
\rhead{\huge     \fontfamily{lmdh}\selectfont     HW \hwnum}

\lfoot{\small CS 70, Spring 2017, HW \hwnum}
\begin{document}

\Question{Sundry} 
\vspace{10pt}
%%%%%%%%%%%%%%%%%%%% SUNDRY PART HERE
\begin{mdframed} \textbf{Solution} 
\item \textit {I, Raymond Feng, certify that all solutions are entirely in my words and that I have not looked at another student's solutions. I have credited all external sources in this write up.}
\item Sherman Luo email - shermanluo@berkeley.edu
\item Credit for this LaTex template goes to anonymous CS70 Piazza user.
\end{mdframed}
%%%%%%%%%%%%%%%%%%%% END SUNDRY
\vfill\pagebreak[3]

%%%%%%%%%%%%%%%%%%%% QUESTIONS START HERE
\Question{Cliques in Random Graphs}

Consider a graph $G(V,E)$ on $n$ vertices which is generated by the following random process: for each pair of vertices $u$ and $v$, we flip a fair coin and place an (undirected) edge between $u$ and $v$ if and only if the coin comes up heads. So for example if $n = 2$, then with probability $1/2$, $G(V,E)$ is the graph consisting of two vertices connected by an edge, and with probability $1/2$ it is the graph consisting of two isolated vertices.

\begin{Parts}
\Part What is the size of the sample space?
\begin{mdframed} \textbf{Solution} \\
$2^{\frac{n(n-1)}{2}}$
\end{mdframed}
\Part A $k$-clique in graph is a set of $k$ vertices which are pairwise adjacent (every pair of vertices is connected by an edge). For example a $3$-clique is a triangle. What is the probability that a particular
set of $k$ vertices forms a $k$-clique? 
\begin{mdframed} \textbf{Solution} \\
$\frac{1}{2^{\frac{k(k-1)}{2}}}$
\end{mdframed}
	\Part Prove that the probability that the graph contains a $k$-clique for $k = 4\ceil{\log n}+1$ is at most
	$1/n$. 
\begin{mdframed} \textbf{Solution} \\
$\Pr[\text{1 or more clicks}]=\frac{\binom{n}{k}}{2^{\frac{k^2-k}{2}}} \leq \frac{n^k}{2^{\frac{k^2-k}{2}}}=(\frac{n}{2^{\frac{k-1}{2}}})^k=(\frac{n}{2^{2\ceil{\log n}}})^k \leq (\frac{n}{2^{2\log n}})^k=(\frac{n}{2^{\log n^2}})^k=(\frac{n}{n^2})^k=(\frac{1}{n})^k \leq \frac{1}{n}$ \qed
\end{mdframed}
\end{Parts}

\Question{Student Request Collector} 

After a long night of debugging, Alvin has just perfected the new homework party/office hour queue system. CS 70 students sign themselves up for the queue, and TAs go through the queue, resolving requests one by one. Unfortunately, our newest TA (let's call him TA Bob) does not understand how to use the new queue: instead of resolving the requests in order, he always uses the Random Student button, which (as the name suggests) chooses a random student in the queue for him. To make matters worse, after helping the student, Bob forgets to click the Resolve button, so the student still remains in the queue! For this problem, assume that there are $n$ total students in the queue.
\begin{Parts}
	\Part Suppose that Bob has already helped $k$ students. What is the probability that the Random Student button will take him to a student who has not already been helped?
\begin{mdframed} \textbf{Solution} \\
$\frac{n-k}{n}$
\end{mdframed}

	\Part Let $X_i^r$ be the event that TA Bob has not helped student $i$ after pressing the Random Student button a total of $r$ times. What is $\Pr[X_i^r]$? Assume that the results of the Random Student button are independent of each other. Now approximate the answer using the inequality $1-x \leq e^{-x}$.
\begin{mdframed} \textbf{Solution} \\
$\Pr[X_i^r] = \Pr[\text{ith not helped}]^r = (\frac{n-1}{n})^r$ \\
$= (1-\frac{1}{n})^r \leq (e^{\frac{-1}{n}})^r = e^{\frac{-r}{n}}$
\end{mdframed}

	\Part Let $T_r$ represent the event that TA Bob presses the Random Student button $r$ times, but still has not been able to help all $n$ students. (In other words, it takes TA Bob longer than $r$ Random Student button presses before he manages to help every student). What is $T_r$ in terms of the events $X_i^r$? (\textit{Hint}: Events are subsets of the probability space $\Omega$, so you should be thinking of set operations...)
\begin{mdframed} \textbf{Solution} \\
$T_r = \bigcup\limits_{i=1}^{n}X_i^r$
\end{mdframed}

	\Part Using your answer for the previous part, what is an upper bound for $\Pr[T_r]$? (You may leave your answer in terms of $\Pr[X_i^r]$. Use the inequality $1-x \leq e^{-x}$ from before.)
\begin{mdframed} \textbf{Solution} \\
$\Pr[T_r] = \bigcup\limits_{i=1}^{n}\Pr[X_i^r] \leq \sum_{i=1}^{n}\Pr[X_i^r]$ \\
$=\sum_{i=1}^{n}e^{\frac{-r}{n}}=ne^{\frac{-r}{n}}$
\end{mdframed}

	\Part Now let $r = \alpha n \ln n$. What is $\Pr[X_i^r]$?
\begin{mdframed} \textbf{Solution} \\
$\frac{1}{n^\alpha}$
\end{mdframed}

	\Part Calculate an upper bound for $\Pr[T_r]$ using the same value of $r$ as before. (This is more formally known as a bound on the tail probability of the distribution of button presses required to help every student. This distribution will be explored in more detail later, in the context of random variables.)
\begin{mdframed} \textbf{Solution} \\
$n^{1-\alpha}$
\end{mdframed}

	\Part What value of $r$ do you need to bound the tail probability by $1/n^2$? In other words, how many button presses are needed so that the probability that TA Bob has not helped every student is at most $1/n^2$?
\begin{mdframed} \textbf{Solution} \\
$r = 3n\ln{n}$
\end{mdframed}
\end{Parts}


\Question{Combinatorial Coins}

Allen and Alvin are flipping coins for fun. Allen flips a fair coin $k$ times and Alvin flips $n-k$ times. In total there are $n$ coin flips. 

\begin{Parts}
	\Part Use a combinatorial proof to show that $$\sum_{i=0}^k \binom{k}{k - i} \binom{n - k}{i} = \binom{n}{k}.$$
\begin{mdframed} \textbf{Solution} \\
LHS: $\binom{k}{k-i}$ is the number of ways for ALlen to get $k-i$ heads, and $\binom{n-k}{i}$ is the number of ways for Alvin to get $i$ heads. The two multiplied are the number of ways to get $k-i+i=k$ heads from $n$ coins. \\
RHS: This is simply the number of ways for $n$ coins to get $k$ heads. \qed
\end{mdframed}

	\Part Prove that the probability that Allen and Alvin flip the same number of heads is equal to the probability that there are a total of $k$ heads.
\begin{mdframed} \textbf{Solution} \\
Define, A: Alvin and Allen get same number of heads. B: Total get $k$ heads. \\
$\Pr[A] = \frac{\sum_{i=1}^{k}\binom{n-k}{i}\binom{k}{i}}{\sum_{i=0}^{n}\binom{n}{i}} \Pr[B] = \frac{\sum_{i=0}^{k}\binom{k}{k-i}\binom{n-k}{i}}{\sum_{i=0}^{n}\binom{n}{i}}$ \\
$\Pr[A] = \Pr[B] = \frac{\sum_{i=1}^{k}\binom{n-k}{i}\binom{k}{i}}{\sum_{i=0}^{n}\binom{n}{i}} = \frac{\sum_{i=0}^{k}\binom{k}{k-i}\binom{n-k}{i}}{\sum_{i=0}^{n}\binom{n}{i}}$ \\
$\Rightarrow \sum_{i=1}^{k}\binom{n-k}{i}\binom{k}{i} = \sum_{i=0}^{k}\binom{k}{k-i}\binom{n-k}{i}$ \qed
\end{mdframed}
\end{Parts}


\Question{Geometric Distribution}

Two faulty machines, $M_1$ and $M_2$, are repeatedly run synchronously in parallel (i.e., both machines execute one run, then both execute a second run, and so on). On each run, $M_1$ fails with probability $p_1$ and $M_2$ fails with probability $p_2$, all failure events being independent. Let the random variables $X_1$, $X_2$ denote the number of runs until the first failure of $M_1$, $M_2$ respectively; thus $X_1$, $X_2$ have geometric distributions with parameters $p_1$, $p_2$ respectively. Let $X$ denote the number of runs until the first failure of \emph{either} machine. Show that $X$ also has a geometric distribution, with parameter $p_1+p_2-p_1p_2$.

\begin{mdframed} \textbf{Solution} \\
$\Pr[X=i] = \Pr[\text{Neither failed for i-1 runs}]\Pr[\text{One or both failed on the ith run}]$ \\
$= ((1-p_1)(1-p_2))^{i-1}(1-(1-p_1)(1-p_2))$ \\
$= (1-p_1-p_2+p_1p_2)^{i-1}(1-1+p_1+p_2+p_1p_2)$ \\
$= (1-p_1-p_2+p_1p_2)^{i-1}(p_1+p_2+p_1p_2) \text{ which is } Geom(p_1+p_2-p_1p_2)$ 
\end{mdframed}


\Question{Poisson Distribution}

\begin{Parts}
	\Part  It is fairly reasonable to model the number of customers entering a shop during a particular hour as a Poisson random variable. Assume that this Poisson random variable $X$ has mean $\lambda$. Suppose that whenever a customer enters the shop they leave the shop without buying anything with probability $p$. Assume that customers act independently, i.e.~you can assume that they each simply flip a biased coin to decide whether to buy anything at all. Let us denote the number of customers that buy something as $Y$ and the number of them that do not buy anything as $Z$ (so $X = Y+Z$). 
	What is the probability that $Y=k$ for a given $k$? How about $\Pr[Z=k]$? Prove that $Y$ and $Z$ are Poisson random variables themselves.

	\textit{Hint}: You can use the identity
	\begin{align*}
	    e^x=\sum_{k=0}^{\infty}\frac{x^k}{k!}.
	\end{align*}
\begin{mdframed} \textbf{Solution} \\
$\Pr[Y=k]=\sum_{i=k}^{\infty}\Pr[X=i]\Pr[Y=k|\text{i people came}]$ \\
$=\sum_{i=k}^{\infty}\frac{\lambda^i}{i!}e^{-\lambda}\binom{i}{k}p^k(1-p)^{i-k}$ \\
$=(\frac{p}{1-p})^{k}e^{-\lambda}\frac{1}{k!}\sum_{i=k}^{\infty}\frac{\lambda^i}{i!}\frac{i!}{(i-k)!}(1-p)^{i}$ \\
$=(\frac{p}{1-p})^{k}e^{-\lambda}\frac{1}{k!}\sum_{i=k}^{\infty}\frac{(\lambda(1-p))^i}{(i-k)!}$ \\
$=(\frac{p}{1-p})^{k}e^{-\lambda}\frac{1}{k!}\sum_{i=0}^{\infty}\frac{(\lambda(1-p))^{i+k}}{i!}$ \\
$=p^{k}\lambda^{k}e^{-\lambda}\frac{1}{k!}\sum_{i=0}^{\infty}\frac{(\lambda(1-p))^{i}}{i!}$ \\
$=(p\lambda)^{k}e^{-\lambda}\frac{1}{k!}e^{\lambda(1-p)}$ \\
$=(p\lambda)^{k}\frac{1}{k!}e^{-p\lambda} \text{ which is } Poiss(p\lambda)$ \\
The calculation for $Pr[Z=k]$ is similar, except that $p$ is replaced by $1-p$. \\
$Pr[Z=k]=\frac{((1-p)\lambda)^k}{k!e^{(1-p)\lambda}} \text{ which is } Poiss((1-p)\lambda)$
\end{mdframed}

	\Part Prove that $Y$ and $Z$ are independent.
\begin{mdframed} \textbf{Solution} 
$\Pr[Y=a,Z=b]=\Pr[X=a+b,Y=a]$ \\
$=\Pr[Y=a|X=a+b]\Pr[a+b]$ \\
$=\binom{a+b}{a}(1-p)^ap^b\frac{\lambda^{a+b}}{(a+b)!e^{-\lambda}}$ \\
$=\frac{(a+b)!}{a!b!}(1-p)^ap^b\frac{\lambda^{a+b}}{(a+b)!e^{-\lambda}}$ \\
$=\frac{1}{a!b!}(1-p)^ap^b\frac{\lambda^{a+b}}{e^{-\lambda}}$ \\
$\frac{((1-p)\lambda)^a}{a!}e^{-(1-p)\lambda}\frac{(p\lambda)^b}{b!}e^{-p\lambda}$ \\
$=\Pr[Y=a]\Pr[Z=b]$ \qed
\end{mdframed}

	\Part Assume that you were given two independent Poisson random variables $X_1, X_2$. Assume that the first has mean $\lambda_1$ and the second has mean $\lambda_2$. Prove that $X_1+X_2$ is a Poisson random variable with mean $\lambda_1+\lambda_2$.

	\textit{Hint}: Recall the binomial theorem.
	\begin{align*}
	    (x + y)^n = \sum_{k=0}^n \binom{n}{k} x^k y^{n-k}
	\end{align*}
\begin{mdframed} \textbf{Solution} \\
$\Pr[X=j]=\Pr[X_1+X_2=j]$ \\
$=\sum_{i=0}^{j}\Pr[X_1=i]\Pr[X_2=j-i]$ \\
$=\sum_{i=0}^{j}\frac{\lambda_1^i}{i!}e^{-\lambda_1}\frac{\lambda_2^{j-i}}{(j-i)!}e^{-\lambda_2}$ \\
$=e^{-\lambda_1^i-\lambda_2^i}\sum_{i=0}^{j}\frac{\lambda_1^i}{i!}\frac{\lambda_2^{j-i}}{(j-i)!}$ \\
$=\frac{e^{-\lambda_1^i-\lambda_2^i}}{j!}\sum_{i=0}^{j}\frac{\lambda_1^i}{i!}\frac{j!\lambda_2^{j-i}}{(j-i)!}$ \\
$=\frac{e^{-\lambda_1^i-\lambda_2^i}}{j!}(\lambda_1+\lambda_2)^j \text{ which is } Poiss(\lambda_1 + \lambda_2)$ \qed

\end{mdframed}
\end{Parts}


\Question{Poisson Coupling}

Consider the following discrete joint distribution for $p \in [0, 1]$.
\begin{align*}
    \Pr(X=0, Y=0) &= 1-p, \\
    \Pr(X=1, Y=y) &= \frac{e^{-p} p^y}{y!}, \qquad y = 1, 2, \dotsc, \\
    \Pr(X=1, Y=0) &= e^{-p} - (1-p), \\
    \Pr(X=x, Y=y) &= 0, \qquad \text{otherwise}.
\end{align*}

\begin{Parts}
    \Part Recall that all valid distributions satisfy two important properties. Argue that this distribution is a valid joint distribution.
\begin{mdframed} \textbf{Solution} \\
The sum of all probabilities add up to 1: \\
$(\sum_{y=1}^{\infty}\frac{e^{-p}p^y}{y!}) + (1-p) + (e^{-p}-(1-p)) = 1$ \\
$(e^{-p}\sum_{y=1}^{\infty}\frac{p^y}{y!}) + (1-p) + (e^{-p}-1+p)) = 1$ \\
$(e^{-p}(e^p - 1) + (e^{-p})) = 1$ \\
$1 = 1$ \qed \\
All probabilities are in the range $[0,1]$: \\
$1-p$, obvious. \\
$e^{-p}\frac{p^y}{y!}$, $1 \geq e^{-p} \leq 1$ and $1 \geq \frac{p^y}{y!} \leq 1$ when $p \leq 1$. Product must be in range $[0,1].$ \\
$e^{-p}-1+p$, which is $\frac{1-e^p(1-p)}{e^p}$. The maximum of $e^p(1-p)$ is 1, which occurs at $p=0$ according to calculus, and the minimum is 0, which occurs when $p=1$. Then, $\frac{1-e^p(1-p)}{e^p}$ is in the range $[0,1]$. 
\qed
\end{mdframed}

    \Part Show that $X$ has the Bernoulli distribution with probability $p$.
\begin{mdframed} \textbf{Solution} \\
$\Pr[X=1]=\sum_{y=1}^{\infty}\Pr[X=1,Y=y+\Pr[X=1,Y=0]$ \\
$=(\sum_{y=1}^{\infty}\Pr[X=1, Y=y])+(e^{-p}-(1-p))$ \\
$=(\sum_{y=1}^{\infty}\frac{e^{-p}p^y}{y!})+(e^{-p}-(1-p))$ \\
$=(e^{-p}(e^p-1))+(e^{-p}-(1-p))$ \\
$=1-e^{-p}+e^{-p}-1+p$ \\
$=p$ \\
$\Pr[X=0] = 0$ \qed
\end{mdframed}

    \Part Show that $Y$ has the Poisson distribution with parameter $\lambda = p$.
\begin{mdframed} \textbf{Solution} \\
$\Pr[Y=y]=\frac{e^{-p}p^y}{y!}$ for $y!=0$. \\
$\Pr[Y=0]=e^{-p}$ \\
But $e^{-p}=\frac{e^{-p}p^0}{0!}$ \\
So, $\Pr[Y=y]=\frac{e^{-p}p^y}{y!}$ for all $y$. \qed
\end{mdframed}

    \Part Show that $\Pr(X \neq Y) \leq p^2$.
\begin{mdframed} \textbf{Solution} 
$\Pr[X \neq Y]=1-\sum_{i=1}^{\infty}\Pr[X=i \cap Y=i]$ \\
$=1-\Pr[X=0,Y=0]-\Pr[X=1,Y=1]$ \\
$=1-(1-p)-e^{-p}p$ \\
$=p-e^{-p}p\leq p^2$ \\
$\Rightarrow 1-e^{-p} \leq p$ \\
$\Rightarrow 1-p \leq e^{-p}$ \\
This inequality was assumed true from a previous problem. \qed
\end{mdframed}
\end{Parts}

Now, let $X_i$, $i = 1, 2, \dotsc$ be a sequence of random variables with probabilities $p_i$, $i = 1, 2, \dotsc$. Similarly, let $Y_i$ be a Poisson random variable with parameter $\lambda = p_i$, $i=1, 2, \dotsc$. The $X_i$ and $Y_i$ are coupled, so that they have the joint distribution described above (with $p = p_i$), but for $i \neq j$, $(X_i, Y_i)$ and $(X_j, Y_j)$ are independent.

We will now introduce a coupling argument which shows that the distribution of $\sum_{i=1}^n X_i$ approaches a Poisson distribution with parameter $\lambda = p_1 + \cdots + p_n$.

\begin{Parts}
    \setcounter{enumi}{4}
    \Part A common way to measure the ``distance'' between two probability distributions is known as the total variation norm, and it is given by
    \begin{align*}
        d(X, Y) &= \frac{1}{2} \sum_{k=0}^\infty |\Pr(X = k) - \Pr(Y = k)|.
    \end{align*}
    Show that $d(X, Y) \leq \Pr(X \neq Y)$. [\textit{Hint}: Use the Law of Total Probability to split up the events according to $\{X = Y\}$ and $\{X \neq Y\}$.]
\begin{mdframed} \textbf{Solution} \\
$\frac{1}{2}\sum_{k=0}^{\infty}|\Pr[X=k]-\Pr[Y=k]|$ \\
$=\frac{1}{2}\sum_{k=0}^{\infty}|\Pr[X=k \cap Y=k]+\Pr[X=k \cap Y \neq k]-\Pr[Y=k \cap X=k]-\Pr[Y=k \cap X \neq k]|$ \\
Because $\Pr[X=k \cap Y=k]=\Pr[Y=k \cap X=k$: \\
$=\frac{1}{2}\sum_{k=0}^{\infty}|\Pr[X=k \cap Y \neq k]-\Pr[Y=k \cap X \neq k]|$ \\
$=\frac{1}{2}\sum_{k=0}^{\infty}|\Pr[X=k \cap Y \neq k]-\Pr[Y=k \cap X \neq k]| \leq \Pr[X \neq Y]$ \\
$\Rightarrow \frac{1}{2}\sum_{k=0}^{\infty}|\Pr[X=k \cap Y \neq k]-\Pr[Y=k \cap X \neq k]| \leq 1-\sum_{i=1}^{\infty}\Pr[X=i \cap Y=i]$ \\
Flip the sign of the minus inside the absolute value, valid because proving something larger also proves the lesser original claim. \\
$\Rightarrow \frac{1}{2}\sum_{k=0}^{\infty}|\Pr[X=k \cap Y \neq k]+\Pr[Y=k \cap X \neq k]| \leq 1-\sum_{i=1}^{\infty}\Pr[X=i \cap Y=i]$ \\
Remove the absolute value because it is impossible for the sum of two probabilities to be negative. \\
$\Rightarrow \frac{1}{2}\sum_{k=0}^{\infty}\Pr[X=k \cap Y \neq k]+\Pr[Y=k \cap X \neq k] \leq 1-\sum_{i=1}^{\infty}\Pr[X=i \cap Y=i]$ \\
$\Rightarrow \frac{1}{2}\sum_{k=0}^{\infty}\Pr[X=k \cap Y \neq k]+\Pr[Y=k \cap X \neq k]+2\Pr[X=k \cap Y=k] \leq 1$ \\
$\Rightarrow \frac{1}{2}\sum_{k=0}^{\infty}(\Pr[X=k \cap Y \neq k]+\Pr[X=k \cap Y=k])+(\Pr[Y=k \cap X \neq k]+\Pr[X=k \cap Y=k]) \leq 1$ \\
$\Rightarrow \frac{1}{2}\sum_{k=0}^{\infty}(\Pr[X=k])+(\Pr[Y=k]) \leq 1$ \\
$\Rightarrow \frac{1}{2}(1+1) \leq 1$ \\
$\Rightarrow 1 \leq 1$ \qed
\end{mdframed}

    \Part Show that $\Pr(\sum_{i=1}^n X_i \neq \sum_{i=1}^n Y_i) \leq \sum_{i=1}^n \Pr(X_i \neq Y_i)$. [\textit{Hint}: Maybe try the Union Bound.]
\begin{mdframed} \textbf{Solution} \\
Claim: $\Pr[\sum_{i=1}^nX_1 \neq \sum_{i=1}^nY_i] \leq \Pr[\bigcup_{i=1}^nX_1 \neq Y_1]$. This is because in order for $\sum_{i=1}^nX_1 \neq \sum_{i=1}^nY_i$ to occur, it must be the case that $\bigcup_{i=1}^nX_1 \neq Y_1$. However, the converse is not true, because there could be pairs that are not equivalent, but the differences cancel each other out. Then, using the union bound: \\
$\Pr[\sum_{i=1}^nX_1 \neq \sum_{i=1}^nY_i] \leq \Pr[\bigcup_{i=1}^nX_i\neq Y_i] \leq \sum_{i=1}^n \Pr(X_i \neq Y_i)$ \qed
\end{mdframed}

    \Part Finally, for the $X_i$ and $Y_i$ defined above, show that $d(\sum_{i=1}^n X_i, \sum_{i=1}^n Y_i) \leq \sum_{i=1}^n p_i^2$.
\begin{mdframed} \textbf{Solution} \\
From part e we can say: \\
$d(\sum_{i=1}^n X_i, \sum_{i=1}^n Y_i) \leq \Pr[\sum_{i=1}^n X_i \neq \sum_{i=1}^n Y_i]$ \\
From part f we can say: \\
$d(\sum_{i=1}^n X_i, \sum_{i=1}^n Y_i) \leq \Pr[\sum_{i=1}^n X_i \neq \sum_{i=1}^n Y_i] \leq \sum_{i=1}^n \Pr(X_i \neq Y_i)$ \\
And finally from part d we can say: \\
$d(\sum_{i=1}^n X_i, \sum_{i=1}^n Y_i) \leq \Pr[\sum_{i=1}^n X_i \neq \sum_{i=1}^n Y_i] \leq \sum_{i=1}^n \Pr(X_i \neq Y_i) \leq \sum_{i=1}^n p_i^2$ \qed
\end{mdframed}
\end{Parts}
%%%%%%%%%%%%%%%%%%%% QUESTIONS END HERE

\end{document}